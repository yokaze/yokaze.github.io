<!DOCTYPE html>
<html lang="ja">
  <head>
    <meta charset="utf-8">
<meta http-equiv="content-language" content="ja">
<meta name="pinterest" content="nopin">
<meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">
<meta name="generator" content="Hugo 0.81.0" />



<link rel="canonical" href="https://yokaze.github.io/2018/01/13/">


    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.6.0/styles/foundation.min.css">
    <title>論文紹介: Feature Learning for Chord Recognition: The Deep Chroma Extractor - yokaze.github.io</title>
    
<meta name="description" content="ISMIR 2016 で発表された、Deep Learning による和音認識の研究論文を読みました。 論文のモチベーション 著者らはまず、和音認識のレビュー論文 [M. McVicar et. al., 2014] を紹介し">

<meta property="og:title" content="論文紹介: Feature Learning for Chord Recognition: The Deep Chroma Extractor - yokaze.github.io">
<meta property="og:type" content="article">
<meta property="og:url" content="https://yokaze.github.io/2018/01/13/">
<meta property="og:image" content="https://yokaze.github.io/images/default.png">
<meta property="og:site_name" content="yokaze.github.io">
<meta property="og:description" content="ISMIR 2016 で発表された、Deep Learning による和音認識の研究論文を読みました。 論文のモチベーション 著者らはまず、和音認識のレビュー論文 [M. McVicar et. al., 2014] を紹介し">
<meta property="og:locale" content="ja_JP">

<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="yokaze.github.io">
<meta name="twitter:url" content="https://yokaze.github.io/2018/01/13/">
<meta name="twitter:title" content="論文紹介: Feature Learning for Chord Recognition: The Deep Chroma Extractor - yokaze.github.io">
<meta name="twitter:description" content="ISMIR 2016 で発表された、Deep Learning による和音認識の研究論文を読みました。 論文のモチベーション 著者らはまず、和音認識のレビュー論文 [M. McVicar et. al., 2014] を紹介し">
<meta name="twitter:image" content="https://yokaze.github.io/images/default.png">


<script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "NewsArticle",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id":"https:\/\/yokaze.github.io\/"
    },
    "headline": "論文紹介: Feature Learning for Chord Recognition: The Deep Chroma Extractor - yokaze.github.io",
    "image": {
      "@type": "ImageObject",
      "url": "https:\/\/yokaze.github.io\/images\/default.png",
      "height": 800,
      "width": 800
    },
    "datePublished": "2018-01-13T20:14:37JST",
    "dateModified": "2018-01-13T20:14:37JST",
    "author": {
      "@type": "Person",
      "name": "yokaze.github.io"
    },
    "publisher": {
      "@type": "Organization",
      "name": "yokaze.github.io",
      "logo": {
        "@type": "ImageObject",
        "url": "https:\/\/yokaze.github.io\/images/logo.png",
        "width": 600,
        "height": 60
      }
    },
    "description": "ISMIR 2016 で発表された、Deep Learning による和音認識の研究論文を読みました。 論文のモチベーション 著者らはまず、和音認識のレビュー論文 [M. McVicar et. al., 2014] を紹介し"
  }
</script>


    <link href="https://yokaze.github.io/css/styles.css" rel="stylesheet">
    

  </head>

  <body>
    
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-83271338-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

    

    <header class="l-header">
      <nav class="navbar navbar-default">
        <div class="container">
          <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://yokaze.github.io/">yokaze.github.io</a>
          </div>

          
          <div id="navbar" class="collapse navbar-collapse">
            
            <ul class="nav navbar-nav navbar-right">
              
              
              <li><a href="/about/">About</a></li>
              
              
              
              <li><a href="/software/">Software</a></li>
              
              
            </ul>
            
          </div>
          

        </div>
      </nav>
    </header>

    <main>
      <div class="container">
        
<div class="row">
  <div class="col-md-8">

    

    <article class="single">
  <header>
    <ul class="p-facts">
      <li><i class="fa fa-calendar" aria-hidden="true"></i><time datetime="2018-01-13T20:14:37JST">Jan 13, 2018</time></li>
      <li><i class="fa fa-bookmark" aria-hidden="true"></i><a href="https://yokaze.github.io/2018/">2018</a></li>
      
    </ul>

    <h1 class="title">論文紹介: Feature Learning for Chord Recognition: The Deep Chroma Extractor</h1>
  </header>

  

  <div class="article-body"><p>ISMIR 2016 で発表された、Deep Learning による和音認識の研究論文を読みました。</p>
<h3 id="論文のモチベーション">論文のモチベーション</h3>
<p>著者らはまず、和音認識のレビュー論文 [M. McVicar et. al., 2014] を紹介し、和音認識が一般に</p>
<ol>
<li>特徴量（クロマベクトル）の抽出</li>
<li>特徴量に対する和音ラベルの割り当て</li>
<li>平滑化、ビート同期などのポストフィルタリング</li>
</ol>
<p>の３ステップからなることを説明しています。
また、同レビュー論文の内容から、認識性能向上のためには特徴量の再設計が必要であると判断しています。</p>
<h3 id="ネットワーク設計と学習">ネットワーク設計と学習</h3>
<p>以下の手順で DNN の入力ベクトルを作成します。</p>
<ol>
<li>モノラルの音響信号を STFT に変換する。フレームサイズは 8192, ホップ長は 4410 (100 ms)。</li>
<li>三角窓を使い対数周波数に変換する。対数周波数軸は 30 &ndash; 5500 Hz, 24 bins/oct (178 bins)。</li>
<li>スペクトログラムの値を対数に変換する。$S_{\mathrm{log}} = \mathrm{log}(1 + S)$ とする。</li>
<li>前後 15 フレーム（1.5 秒分）を結合し、各時間フレームの特徴量（入力ベクトル）とする。DNN の入力次元数は $178 \times 15 = 2670$。</li>
</ol>
<p>これを 3 + 1 層の全結合ネットワークで学習します。
ネットワークの大きさは 2670 <span>&ndash;</span> 512 <span>&ndash;</span> 512 <span>&ndash;</span> 512 <span>&ndash;</span> 12 で、
最初の三層は ReLU (rectified linear unit)、出力層は sigmoid 関数となっています。
コスト関数は正解和音ラベルとの相互エントロピーで、ADAM を使った勾配法により学習を進めます。</p>
<p>実験結果では、提案法が通常のクロマベクトルと比較して数ポイント性能が向上すること、中間層が和音ラベルごとのアテンションマップとして解釈できることなどを説明しています。</p>
<h3 id="感想">感想</h3>
<p>挑戦的なテーマでありながら、手法の選択やパラメータの設定理由などが詳しく書かれており、とても勉強になる論文でした。</p>
<p>
  <a href="https://twitter.com/share?ref_src=twsrc%5Etfw" class="twitter-share-button" data-lang="ja" data-show-count="false">Tweet</a>
  <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</p>

<h2 id="参考文献">参考文献</h2>
<ul>
<li>
<p>F. Korzeniowski and G. Widmer, &ldquo;Feature Learning for Chord Recognition: The Deep Chroma Extractor&rdquo;, in <em>Proc. ISMIR</em>, 2016, pp. 37&ndash;43.<br />
<span style="word-break: break-all;">
<a href="http://www.cp.jku.at/research/papers/Korzeniowski_ISMIR_2016.pdf">http://www.cp.jku.at/research/papers/Korzeniowski_ISMIR_2016.pdf</a>
</span></p>
</li>
<li>
<p>The Deep Chroma Extractor<br />
<span style="word-break: break-all;">
<a href="http://fdlm.github.io/post/deepchroma/">http://fdlm.github.io/post/deepchroma/</a>
</span></p>
</li>
<li>
<p>M. McVicar et. al., &ldquo;Automatic Chord Estimation from Audio: A Review of the State of the Art&rdquo;, <em>IEEE/ACM Trans. on ASLP</em>, vol. 22, no. 2, pp. 556&ndash;575, 2014.<br />
<span style="word-break: break-all;">
<a href="http://ieeexplore.ieee.org/document/6705583/">http://ieeexplore.ieee.org/document/6705583/</a>
</span></p>
</li>
<li>
<p>MIREX2016コード推定タスクで勝ったディープラーニングの論文を読む<br />
<span style="word-break: break-all;">
<a href="http://xiao-ming.digick.jp/mir/jkudeepace/">http://xiao-ming.digick.jp/mir/jkudeepace/</a>
</span></p>
</li>
<li>
<p>The International Society of Music Information Retrieval<br />
<span style="word-break: break-all;">
<a href="http://www.ismir.net/">http://www.ismir.net/</a>
</span></p>
</li>
</ul>
</div>
 
  <br />

  <footer class="article-footer">
    
    
    
    <section class="bordered">
      <header>
        <div class="panel-title">CATEGORIES</div>
      </header>
      <div>
        <ul class="p-terms">
          
          <li><a href="https://yokaze.github.io/categories/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/">機械学習</a></li>
          
        </ul>
      </div>
    </section>
    
    
  </footer>

</article>


    
  </div>

  <div class="col-md-4">
    
<aside class="l-sidebar">

  
  
  <section class="panel panel-default">
    <div class="panel-heading">
      <div class="panel-title">RELATED POSTS</div>
    </div>
    <div class="list-group">
      
      <a href="https://yokaze.github.io/2018/01/04/" class="list-group-item">論文紹介: Music Emotion Recognition: A State of the Art Review</a>
      
      <a href="https://yokaze.github.io/2019/09/12/" class="list-group-item">Sliced Wasserstein GMM を実装してみた</a>
      
      <a href="https://yokaze.github.io/2019/08/30/" class="list-group-item">TensorFlow 2.0 で混合ガウス分布 (GMM) を推定する</a>
      
      <a href="https://yokaze.github.io/2019/08/23/" class="list-group-item">線形位相空間の原点の近傍が併呑集合であることを証明する</a>
      
      <a href="https://yokaze.github.io/2019/08/12/" class="list-group-item">TensorFlow 2.0 で非負値行列因子分解 (NMF) を解く</a>
      
      <a href="https://yokaze.github.io/2019/08/07/" class="list-group-item">TensorFlow 2.0 で Variable を ndarray に変換する</a>
      
      <a href="https://yokaze.github.io/2019/07/13/" class="list-group-item">Monge-Kantorovich の問題を SciPy で解く</a>
      
      <a href="https://yokaze.github.io/2019/07/12/" class="list-group-item">Wasserstein 計量が距離関数になることを証明する</a>
      
      <a href="https://yokaze.github.io/2017/10/13/" class="list-group-item">機械学習で役立つ数学参考書のリスト</a>
      
      <a href="https://yokaze.github.io/2017/10/11/" class="list-group-item">EM アルゴリズム</a>
      
    </div>
  </section>
  

  
  <section class="panel panel-default">
    <div class="panel-heading">
      <div class="panel-title">CATEGORY</div>
    </div>
    <div class="list-group">
      
      <a href="https://yokaze.github.io/categories/%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0" class="list-group-item">プログラミング</a>
      
      <a href="https://yokaze.github.io/categories/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92" class="list-group-item">機械学習</a>
      
      <a href="https://yokaze.github.io/categories/%E5%86%99%E7%9C%9F" class="list-group-item">写真</a>
      
    </div>
  </section>
  

  <section class="panel panel-default">
    <div class="panel-heading">
      <div class="panel-title">AUTHOR</div>
    </div>
    <div class="list-group">
      <a href="https://yokaze.github.io//about/" class="list-group-item">Rue Yokaze&ensp;<i class="fa fa-camera-retro" aria-hidden="true"></i></a>
    </div>
  </section>

</aside>

  </div>
</div>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config
  ({
    tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']], processEscapes: true },
    TeX: { equationNumbers: { autoNumber: "AMS" }}
  });
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_SVG"></script>


      </div>
    </main>

    <footer class="l-footer">
      <div class="container">
        <p>&copy; <a href="https://yokaze.github.io//about/">Rue Yokaze</a> <i class="fa fa-twitter" aria-hidden="true"></i>&nbsp;<i class="fa fa-github-alt" aria-hidden="true"></i>&nbsp;<i class="fa fa-instagram" aria-hidden="true"></i></p>
        <aside>
          <p>Powered by <a href="https://gohugo.io/">Hugo</a>.</p>
          <p><a href="https://github.com/dim0627/hugo_theme_beg">Beg</a> designed by <a href="http://yet.unresolved.xyz/">Daisuke Tsuji</a>.</p>
        </aside>
      </div>
    </footer>

    <script src="//code.jquery.com/jquery-3.1.1.min.js"></script>
    <script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.6.0/highlight.min.js"></script>
    
    <script>hljs.initHighlightingOnLoad();</script>
  </body>
</html>
